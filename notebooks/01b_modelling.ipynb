{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">‚òÅÔ∏è - Cloudy regions segmentation üë®‚Äçüíªüî¨</h1>\n",
    "\n",
    "<h2 align=\"center\">Modelling - 1st iteration</h2>\n",
    "<p style=\"text-align:center\">\n",
    "   Thomas Bury, Afonso Alves, Daniel Staudegger<br>\n",
    "   Allianz<br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea start with classification and iteratively select more complex model: \n",
    "- Standard Classification of 15 classes (Random Forest and Neural Networks)\n",
    "- 4 different classifier models for each class\n",
    "- Object Detection?\n",
    "- Semantic Segmentation for one class\n",
    "- Semantic Segmentation for multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** üí≠: do a single NN to predict all four label maks? Or do 4 NN to predict each label mask individually, and then aggregate the predctions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important detail that may affect the perfomance of the NN:** training label masks were generated by scientists by drawing **rectangular shaped bounding boxes**. Alternatively, the label mask could just have been the actual clouds masks belonging the cloud type (instead of retangular, the mask would have the exact shape of the clouds, with many empty spaces between each mask portion - \"where you can see the sea\" areas). This means the mask label we have contains some \"empty information area\", corresponding to the cloudless part of the retangular bounding box drawn by the scientist. I don't have enough experience to say that this will make the training harder (e.g. higher runtime and/or worst performance), but at least will affect it, because the NN has to figure out that the cloudless pixels in the proximity of cloud pixels may also belong to the true label mask, and that the resulting prediction typically has a retangular shape. **Do you guys agree with this assessment?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment about the problem nature:** in reality, there are \"4x2=8 problems\" that the NN needs to address: for each possible cloud type, decide if the cloud type is present in the image (kinda like a \"binary problem\"), and if so, decide on a cloud mask. Perhaps this predicament can inform the NN arquitecture (like a \"binary decision layer\" + \"masks guess layer\") ?? Research more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other comment about the problem nature:** we could think of problem as a **multiclass softmax problem**, with each pixel having 5 possible classes (None, 'Gravel', 'Flower', 'Sugar', 'Fish'). Nonetheless, we have seen that the cloud bboxes drawn by the scientists can overlap (even if from different cloud labels). This would mean that a pixel can have more than one class associated (e.g y = ('Gravel','Fish') - overlap of two different cloud types in this pixel), making it a **multilabel problem**, with a weird restriction: the None class, if present, can't be associated to other classes (a pixel can't be class None and \"Gravel\" at the same time), at least if we consider None (empty label) a concrete possible class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate pyreidolia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scicomap as sc\n",
    "import matplotlib as mpl\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#To get a progress bar for long loops:\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep\n",
    "\n",
    "#Machine Learning and Deep Learning Packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Custom package for the project, save all the functions into appropriate sub-packages\n",
    "from pyreidolia.plot import set_my_plt_style, plot_cloud, plot_rnd_cloud, draw_label_only\n",
    "from pyreidolia.mask import bounding_box, rle_to_mask, get_binary_mask_sum, mask_to_rle\n",
    "from pyreidolia.img import get_resolution_sharpness\n",
    "\n",
    "#Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport pyreidolia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config file for the paths\n",
    "To avoid to hardcode the paths in a versioned file, let's create a `paths.yml` which will **not** be versioned. So that the paths are not overwritten when we pull or merge from the GitHub repo. The `paths.yml` should have a structure like:\n",
    "\n",
    "```yml\n",
    "# data\n",
    "data:\n",
    "  test: \"C:/Users/xtbury/Documents/Projects/segmentation_cloudy_regions/data/test_images/\"\n",
    "  train: \"C:/Users/xtbury/Documents/Projects/segmentation_cloudy_regions/data/train_images/\"\n",
    "  docs: \"C:/Users/xtbury/Documents/Projects/segmentation_cloudy_regions/data/\"\n",
    "\n",
    "# Path to store all notebooks, ideally not versioned\n",
    "notebooks: \"C:/Users/xtbury/Documents/Projects/segmentation_cloudy_regions/notebooks/\"\n",
    "\n",
    "# Path to store all outputs (correlations, jsons, excel, etc)\n",
    "output: \"C:/Users/xtbury/Documents/Projects/segmentation_cloudy_regions/output/\"\n",
    "\n",
    "# Path to store all python scripts, for versioning\n",
    "scripts: \"C:/Users/xtbury/Documents/Projects/segmentation_cloudy_regions/scripts/\"\n",
    "\n",
    "# Path to studies\n",
    "studies: \"C:/Users/xtbury/Documents/Projects/segmentation_cloudy_regions/studies/\"\n",
    "\n",
    "# Path to reports\n",
    "reports: \"C:/Users/xtbury/Documents/Projects/Pyreidolia/reports/\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_print(df):\n",
    "    return print(df.to_string().replace('\\n', '\\n\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the paths.yml config file?C:/Users/A00006617/OneDrive - Allianz Global Investors/Documents/Allianz Data Scientist/Project/Pyreidolia/paths.yml\n",
      "{'data': {'docs': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "                  'Investors/Documents/Allianz Data '\n",
      "                  'Scientist/Project/Pyreidolia/data/',\n",
      "          'test': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "                  'Investors/Documents/Allianz Data '\n",
      "                  'Scientist/Project/Pyreidolia/data/test_images/',\n",
      "          'train': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "                   'Investors/Documents/Allianz Data '\n",
      "                   'Scientist/Project/Pyreidolia/data/train_images/'},\n",
      " 'notebooks': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "              'Investors/Documents/Allianz Data '\n",
      "              'Scientist/Project/Pyreidolia/notebooks/',\n",
      " 'output': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "           'Investors/Documents/Allianz Data '\n",
      "           'Scientist/Project/Pyreidolia/output/',\n",
      " 'reports': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "            'Investors/Documents/Allianz Data '\n",
      "            'Scientist/Project/Pyreidolia/reports/',\n",
      " 'scripts': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "            'Investors/Documents/Allianz Data '\n",
      "            'Scientist/Project/Pyreidolia/scripts/',\n",
      " 'studies': 'C:/Users/A00006617/OneDrive - Allianz Global '\n",
      "            'Investors/Documents/Allianz Data '\n",
      "            'Scientist/Project/Pyreidolia/studies/'}\n"
     ]
    }
   ],
   "source": [
    "# Where is my yaml ? \"C:/Users/A00006617/OneDrive - Allianz Global Investors/Documents/Allianz Data Scientist/Project/Pyreidolia/paths.yml\"\n",
    "\n",
    "paths_yml = input(\"where is the paths.yml config file?\")\n",
    "with open(paths_yml, \"r+\") as ymlfile:\n",
    "    path_dic = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "pprint(path_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = path_dic['data']['docs'] + 'train.csv'\n",
    "train_pq_path = path_dic['data']['docs'] + \"train_info_clean.parquet\"\n",
    "train_data = path_dic['data']['train'] \n",
    "test_data = path_dic['data']['test'] \n",
    "report_path = path_dic['reports']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned training meta-data\n",
    "train_doc = pd.read_parquet(train_pq_path)\n",
    "train_doc.head(4)\n",
    "#and convert to grouped-masks dataframe to obtain one row per image_id\n",
    "grouped_labels = train_doc.loc[train_doc.is_mask, :].groupby('image_id').agg({'label': lambda x: list(x)})\n",
    "grouped_labels[\"label_comb\"] = grouped_labels[\"label\"].str.join(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load X_train2 and X_valid2 - containing rows of arrays with the pixels of the pre-processed images from train_doc\n",
    "# already split in validation and test set\n",
    "with open(path_dic['data']['docs'] + 'X_train2.npy', 'rb') as f:\n",
    "    X_train2 = np.load(f)\n",
    "\n",
    "with open(path_dic['data']['docs'] + 'X_valid2.npy', 'rb') as f:\n",
    "    X_valid2 = np.load(f)\n",
    "    \n",
    "#we also load the image_ids of train and test\n",
    "with open(path_dic['data']['docs'] + 'Img_train2.pkl', 'rb') as f:\n",
    "    Img_train2 = pd.read_pickle(f)\n",
    "\n",
    "with open(path_dic['data']['docs'] + 'Img_valid2.pkl', 'rb') as f:\n",
    "    Img_valid2 = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load y_train2_IC, y_valid2_IC\n",
    "with open(path_dic['data']['docs'] + 'y_train2_IC.npy', 'rb') as f:\n",
    "    y_train2_IC = np.load(f)\n",
    "    \n",
    "with open(path_dic['data']['docs'] + 'y_valid2_IC.npy', 'rb') as f:\n",
    "    y_valid2_IC = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modeling - Classification (15 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by simplifying the problem and converting it to a standard classification problem. Since standard algorithms cannot detect multiple objects per image, we treat the problem as a classifcation problem with 15 classes (= number of unique label combinations). \\\n",
    "We then try a Random Forest as well as a CNNs with different architecture for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fish', 'Fish-Flower', 'Fish-Flower-Gravel',\n",
       "       'Fish-Flower-Gravel-Sugar', 'Fish-Flower-Sugar', 'Fish-Gravel',\n",
       "       'Fish-Gravel-Sugar', 'Fish-Sugar', 'Flower', 'Flower-Gravel',\n",
       "       'Flower-Gravel-Sugar', 'Flower-Sugar', 'Gravel', 'Gravel-Sugar',\n",
       "       'Sugar'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2_labels = Img_train2.merge(grouped_labels, on = 'image_id', how = 'left')['label_comb']\n",
    "y_valid2_labels = Img_valid2.merge(grouped_labels, on = 'image_id', how = 'left')['label_comb']\n",
    "#Encode the 15 classes to 0-15 with Label-Encoder\n",
    "le = LabelEncoder()\n",
    "y_train2 = le.fit_transform(y_train2_labels)\n",
    "y_valid2 = le.transform(y_valid2_labels)\n",
    "\n",
    "#We see that indeed all label combinations where present and encoded\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RandomForest model\n",
    "clf_rf = RandomForestClassifier(n_jobs = -1)\n",
    "clf_rf.fit(X_train2, y_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        70\n",
      "           1       0.00      0.00      0.00        75\n",
      "           2       0.00      0.00      0.00        44\n",
      "           3       0.00      0.00      0.00        66\n",
      "           4       0.00      0.00      0.00        77\n",
      "           5       0.00      0.00      0.00        62\n",
      "           6       0.13      0.37      0.19       190\n",
      "           7       0.10      0.05      0.07       128\n",
      "           8       0.07      0.01      0.02        82\n",
      "           9       0.00      0.00      0.00        38\n",
      "          10       0.06      0.01      0.02       104\n",
      "          11       0.10      0.05      0.07       111\n",
      "          12       0.00      0.00      0.00        77\n",
      "          13       0.13      0.43      0.20       176\n",
      "          14       0.04      0.02      0.03        86\n",
      "\n",
      "    accuracy                           0.12      1386\n",
      "   macro avg       0.04      0.06      0.04      1386\n",
      "weighted avg       0.06      0.12      0.07      1386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Winapp\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "y_pred_rf = clf_rf.predict(X_valid2)\n",
    "print(classification_report(y_valid2, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unexpectedly quite a poor performance. Let's see if a Neural Network can do any better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train2, dtype = 'int') # Veiller √† n'ex√©cuter cette instruction qu'une seule fois\n",
    "y_valid_cat = to_categorical(y_valid2, dtype = 'int')   # Veiller √† n'ex√©cuter cette instruction qu'une seule fois\n",
    "\n",
    "num_pixels = X_train2.shape[1]\n",
    "num_classes = y_train_cat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Network Architecture\n",
    "inputs = Input(shape = (num_pixels), name = \"Input\")\n",
    "dense1 = Dense(units = 20, activation = \"tanh\", name = \"first_layer\")\n",
    "dense2 = Dense(units = num_classes, activation = \"softmax\", name = \"second_layer\")\n",
    "x = dense1(inputs)\n",
    "outputs = dense2(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "104/104 [==============================] - 4s 39ms/step - loss: 2.7338 - accuracy: 0.1199 - val_loss: 2.6432 - val_accuracy: 0.1166\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 4s 38ms/step - loss: 2.6148 - accuracy: 0.1289 - val_loss: 2.6404 - val_accuracy: 0.1166\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 2.6116 - accuracy: 0.1328 - val_loss: 2.6423 - val_accuracy: 0.1298\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 4s 39ms/step - loss: 2.6104 - accuracy: 0.1358 - val_loss: 2.6435 - val_accuracy: 0.1298\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 2.6112 - accuracy: 0.1316 - val_loss: 2.6443 - val_accuracy: 0.1298\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 4s 39ms/step - loss: 2.6107 - accuracy: 0.1361 - val_loss: 2.6427 - val_accuracy: 0.1166\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 4s 39ms/step - loss: 2.6113 - accuracy: 0.1340 - val_loss: 2.6434 - val_accuracy: 0.1166\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 4s 39ms/step - loss: 2.6107 - accuracy: 0.1325 - val_loss: 2.6414 - val_accuracy: 0.1166\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 4s 39ms/step - loss: 2.6118 - accuracy: 0.1328 - val_loss: 2.6433 - val_accuracy: 0.1166\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 4s 39ms/step - loss: 2.6108 - accuracy: 0.1325 - val_loss: 2.6442 - val_accuracy: 0.1166\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 2.6106 - accuracy: 0.1340 - val_loss: 2.6400 - val_accuracy: 0.1166\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 2.6115 - accuracy: 0.1310 - val_loss: 2.6452 - val_accuracy: 0.1166\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 2.6113 - accuracy: 0.1304 - val_loss: 2.6412 - val_accuracy: 0.1166\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 5s 43ms/step - loss: 2.6098 - accuracy: 0.1340 - val_loss: 2.6415 - val_accuracy: 0.1298\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 5s 44ms/step - loss: 2.6111 - accuracy: 0.1205 - val_loss: 2.6424 - val_accuracy: 0.1166\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 2.6103 - accuracy: 0.1289 - val_loss: 2.6463 - val_accuracy: 0.1166\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 4s 43ms/step - loss: 2.6111 - accuracy: 0.1331 - val_loss: 2.6444 - val_accuracy: 0.1166\n",
      "Epoch 18/20\n",
      "104/104 [==============================] - 4s 43ms/step - loss: 2.6109 - accuracy: 0.1361 - val_loss: 2.6392 - val_accuracy: 0.1298\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 4s 40ms/step - loss: 2.6112 - accuracy: 0.1298 - val_loss: 2.6449 - val_accuracy: 0.1166\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 4s 41ms/step - loss: 2.6108 - accuracy: 0.1358 - val_loss: 2.6436 - val_accuracy: 0.1298\n"
     ]
    }
   ],
   "source": [
    "#Model Training\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "training_history = model.fit(X_train2, y_train_cat, \n",
    "                             epochs = 20, \n",
    "                             batch_size = 32, \n",
    "                             validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        70\n",
      "           1       0.00      0.00      0.00        75\n",
      "           2       0.00      0.00      0.00        44\n",
      "           3       0.00      0.00      0.00        66\n",
      "           4       0.00      0.00      0.00        77\n",
      "           5       0.00      0.00      0.00        62\n",
      "           6       0.14      1.00      0.24       190\n",
      "           7       0.00      0.00      0.00       128\n",
      "           8       0.00      0.00      0.00        82\n",
      "           9       0.00      0.00      0.00        38\n",
      "          10       0.00      0.00      0.00       104\n",
      "          11       0.00      0.00      0.00       111\n",
      "          12       0.00      0.00      0.00        77\n",
      "          13       0.00      0.00      0.00       176\n",
      "          14       0.00      0.00      0.00        86\n",
      "\n",
      "    accuracy                           0.14      1386\n",
      "   macro avg       0.01      0.07      0.02      1386\n",
      "weighted avg       0.02      0.14      0.03      1386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "y_pred_dnn  = model.predict(X_valid2)\n",
    "y_pred_dnn_class = np.argmax(y_pred_dnn, axis = 1)  #Turn probabilities into vector with the predicted class\n",
    "print(classification_report(y_valid2, y_pred_dnn_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better, but still poor. \n",
    "We note the model was succesful by focusing on only one specific class. Hence we next try to change the approach and see if a classifier can identify the existence of a specific cloud type in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling - Classification (1 class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to a different approach, where we create a separate classifier for each label and see if this way we can get a higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the 4 different training and validation vectors - the order of the labels come from 01a_preprocessing notebook\n",
    "y_train2_IC_fish = y_train2_IC[:,0]\n",
    "y_train2_IC_flower = y_train2_IC[:,1]\n",
    "y_train2_IC_gravel = y_train2_IC[:,2]\n",
    "y_train2_IC_sugar = y_train2_IC[:,3]\n",
    "\n",
    "y_valid2_IC_fish = y_valid2_IC[:,0]\n",
    "y_valid2_IC_flower = y_valid2_IC[:,1]\n",
    "y_valid2_IC_gravel = y_valid2_IC[:,2]\n",
    "y_valid2_IC_sugar = y_valid2_IC[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We create 4 Random Forest Classifiers - one for each label\n",
    "clf_rf_fish = RandomForestClassifier(n_jobs = -1)\n",
    "clf_rf_flower = RandomForestClassifier(n_jobs = -1)\n",
    "clf_rf_gravel = RandomForestClassifier(n_jobs = -1)\n",
    "clf_rf_sugar = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "clf_rf_fish.fit(X_train2, y_train2_IC_fish)\n",
    "clf_rf_flower.fit(X_train2, y_train2_IC_flower)\n",
    "clf_rf_gravel.fit(X_train2, y_train2_IC_gravel)\n",
    "clf_rf_sugar.fit(X_train2, y_train2_IC_sugar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.52      0.49       674\n",
      "           1       0.49      0.44      0.47       712\n",
      "\n",
      "    accuracy                           0.48      1386\n",
      "   macro avg       0.48      0.48      0.48      1386\n",
      "weighted avg       0.48      0.48      0.48      1386\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.88      0.69       789\n",
      "           1       0.44      0.13      0.20       597\n",
      "\n",
      "    accuracy                           0.55      1386\n",
      "   macro avg       0.51      0.50      0.45      1386\n",
      "weighted avg       0.52      0.55      0.48      1386\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.41      0.42       629\n",
      "           1       0.54      0.58      0.56       757\n",
      "\n",
      "    accuracy                           0.50      1386\n",
      "   macro avg       0.49      0.49      0.49      1386\n",
      "weighted avg       0.50      0.50      0.50      1386\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.02      0.04       448\n",
      "           1       0.68      0.98      0.80       938\n",
      "\n",
      "    accuracy                           0.67      1386\n",
      "   macro avg       0.51      0.50      0.42      1386\n",
      "weighted avg       0.57      0.67      0.55      1386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions and model evaluation\n",
    "y_pred_rf_fish = clf_rf_fish.predict(X_valid2)\n",
    "y_pred_rf_flower = clf_rf_flower.predict(X_valid2)\n",
    "y_pred_rf_gravel = clf_rf_gravel.predict(X_valid2)\n",
    "y_pred_rf_sugar = clf_rf_sugar.predict(X_valid2)\n",
    "print(classification_report(y_valid2_IC_fish, y_pred_rf_fish),\n",
    "     classification_report(y_valid2_IC_flower, y_pred_rf_flower),\n",
    "     classification_report(y_valid2_IC_gravel, y_pred_rf_gravel),\n",
    "     classification_report(y_valid2_IC_sugar, y_pred_rf_sugar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model already gives better results - in particular for Sugar it seems to work not too bad. We remember that Sugar are typically wide-spread cloud structures, so perhaps the classifier can spot large structures of Clouds fairly well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = X_train2.shape[1]\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by choosing the same simple Network Architecture as above\n",
    "inputs = Input(shape = (num_pixels), name = \"Input\")\n",
    "dense1 = Dense(units = 20, activation = \"relu\", name = \"first_layer\")\n",
    "dense2 = Dense(units = num_classes, activation = \"sigmoid\", name = \"second_layer\")\n",
    "x = dense1(inputs)\n",
    "outputs = dense2(x)\n",
    "\n",
    "model_fish = Model(inputs = inputs, outputs = outputs)\n",
    "model_flower = Model(inputs = inputs, outputs = outputs)\n",
    "model_gravel = Model(inputs = inputs, outputs = outputs)\n",
    "model_sugar = Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 1295.3586 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.4904\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6932 - val_accuracy: 0.4904\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6933 - val_accuracy: 0.4904\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6933 - val_accuracy: 0.4904\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6933 - val_accuracy: 0.4904\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 3s 26ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 3s 28ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 3s 28ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6935 - val_accuracy: 0.4904\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6935 - val_accuracy: 0.4904\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 3s 28ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 18/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4904\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6935 - val_accuracy: 0.4904\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6935 - val_accuracy: 0.4904\n",
      "Epoch 1/20\n",
      "104/104 [==============================] - 4s 34ms/step - loss: 0.6894 - accuracy: 0.5751 - val_loss: 0.6875 - val_accuracy: 0.5745\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.6864 - accuracy: 0.5751 - val_loss: 0.6854 - val_accuracy: 0.5745\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.6846 - accuracy: 0.5751 - val_loss: 0.6840 - val_accuracy: 0.5745\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6834 - accuracy: 0.5751 - val_loss: 0.6831 - val_accuracy: 0.5745\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 3s 28ms/step - loss: 0.6828 - accuracy: 0.5751 - val_loss: 0.6826 - val_accuracy: 0.5745\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6824 - accuracy: 0.5751 - val_loss: 0.6823 - val_accuracy: 0.5745\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6822 - accuracy: 0.5751 - val_loss: 0.6822 - val_accuracy: 0.5745\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6820 - accuracy: 0.5751 - val_loss: 0.6821 - val_accuracy: 0.5745\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 3s 26ms/step - loss: 0.6819 - accuracy: 0.5751 - val_loss: 0.6821 - val_accuracy: 0.5745\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6819 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6819 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6818 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6819 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 3s 26ms/step - loss: 0.6818 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 3s 26ms/step - loss: 0.6819 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 3s 26ms/step - loss: 0.6818 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6818 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 18/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6819 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6818 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6818 - accuracy: 0.5751 - val_loss: 0.6820 - val_accuracy: 0.5745\n",
      "Epoch 1/20\n",
      "104/104 [==============================] - 4s 32ms/step - loss: 0.7073 - accuracy: 0.4787 - val_loss: 0.7077 - val_accuracy: 0.4627\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 3s 26ms/step - loss: 0.7020 - accuracy: 0.4787 - val_loss: 0.7028 - val_accuracy: 0.4627\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6986 - accuracy: 0.4787 - val_loss: 0.6990 - val_accuracy: 0.4627\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6962 - accuracy: 0.4787 - val_loss: 0.6964 - val_accuracy: 0.4627\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6947 - accuracy: 0.4787 - val_loss: 0.6947 - val_accuracy: 0.4627\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6938 - accuracy: 0.4787 - val_loss: 0.6935 - val_accuracy: 0.4627\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6932 - accuracy: 0.4979 - val_loss: 0.6927 - val_accuracy: 0.5373\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6928 - accuracy: 0.5213 - val_loss: 0.6921 - val_accuracy: 0.5373\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6926 - accuracy: 0.5213 - val_loss: 0.6918 - val_accuracy: 0.5373\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6924 - accuracy: 0.5213 - val_loss: 0.6915 - val_accuracy: 0.5373\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 3s 25ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6913 - val_accuracy: 0.5373\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6912 - val_accuracy: 0.5373\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 3s 28ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6911 - val_accuracy: 0.5373\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6910 - val_accuracy: 0.5373\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6910 - val_accuracy: 0.5373\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6910 - val_accuracy: 0.5373\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6909 - val_accuracy: 0.5373\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 3s 28ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6909 - val_accuracy: 0.5373\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6909 - val_accuracy: 0.5373\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 3s 27ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6909 - val_accuracy: 0.5373\n",
      "Epoch 1/20\n",
      "104/104 [==============================] - 4s 33ms/step - loss: 0.6721 - accuracy: 0.6794 - val_loss: 0.6685 - val_accuracy: 0.6635\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6604 - accuracy: 0.6794 - val_loss: 0.6595 - val_accuracy: 0.6635\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6512 - accuracy: 0.6794 - val_loss: 0.6527 - val_accuracy: 0.6635\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6444 - accuracy: 0.6794 - val_loss: 0.6478 - val_accuracy: 0.6635\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6393 - accuracy: 0.6794 - val_loss: 0.6445 - val_accuracy: 0.6635\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6357 - accuracy: 0.6794 - val_loss: 0.6422 - val_accuracy: 0.6635\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6331 - accuracy: 0.6794 - val_loss: 0.6407 - val_accuracy: 0.6635\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6312 - accuracy: 0.6794 - val_loss: 0.6398 - val_accuracy: 0.6635\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 0.6299 - accuracy: 0.6794 - val_loss: 0.6392 - val_accuracy: 0.6635\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6290 - accuracy: 0.6794 - val_loss: 0.6389 - val_accuracy: 0.6635\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 0.6284 - accuracy: 0.6794 - val_loss: 0.6388 - val_accuracy: 0.6635\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 0.6281 - accuracy: 0.6794 - val_loss: 0.6387 - val_accuracy: 0.6635\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 3s 33ms/step - loss: 0.6278 - accuracy: 0.6794 - val_loss: 0.6387 - val_accuracy: 0.6635\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6276 - accuracy: 0.6794 - val_loss: 0.6388 - val_accuracy: 0.6635\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 0.6275 - accuracy: 0.6794 - val_loss: 0.6389 - val_accuracy: 0.6635\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6275 - accuracy: 0.6794 - val_loss: 0.6389 - val_accuracy: 0.6635\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 0.6274 - accuracy: 0.6794 - val_loss: 0.6390 - val_accuracy: 0.6635\n",
      "Epoch 18/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6274 - accuracy: 0.6794 - val_loss: 0.6391 - val_accuracy: 0.6635\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.6274 - accuracy: 0.6794 - val_loss: 0.6391 - val_accuracy: 0.6635\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.6274 - accuracy: 0.6794 - val_loss: 0.6391 - val_accuracy: 0.6635\n"
     ]
    }
   ],
   "source": [
    "model_fish.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "model_flower.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "model_gravel.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "model_sugar.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "training_history_fish = model_fish.fit(X_train2, y_train2_IC_fish, epochs = 20, batch_size = 32, validation_split = 0.2)\n",
    "training_history_flower = model_flower.fit(X_train2, y_train2_IC_flower, epochs = 20, batch_size = 32, validation_split = 0.2)\n",
    "training_history_gravel = model_gravel.fit(X_train2, y_train2_IC_gravel, epochs = 20, batch_size = 32, validation_split = 0.2)\n",
    "training_history_sugar = model_sugar.fit(X_train2, y_train2_IC_sugar, epochs = 20, batch_size = 32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish model score accuracy: 51.37%\n",
      "Flower model score accuracy: 43.07%\n",
      "Gravel model score accuracy: 54.62%\n",
      "Sugar model score accuracy: 67.68%\n"
     ]
    }
   ],
   "source": [
    "# Prediction and model evaluation\n",
    "y_pred_dnn_fish = model_fish.predict(X_valid2)\n",
    "y_pred_dnn_flower = model_flower.predict(X_valid2)\n",
    "y_pred_dnn_gravel = model_gravel.predict(X_valid2)\n",
    "y_pred_dnn_sugar = model_sugar.predict(X_valid2)\n",
    "\n",
    "score_fish = model_fish.evaluate(X_valid2, y_valid2_IC_fish, verbose=0)\n",
    "score_flower = model_flower.evaluate(X_valid2, y_valid2_IC_flower, verbose=0)\n",
    "score_gravel = model_gravel.evaluate(X_valid2, y_valid2_IC_gravel, verbose=0)\n",
    "score_sugar = model_sugar.evaluate(X_valid2, y_valid2_IC_sugar, verbose=0)\n",
    "\n",
    "print(\"Fish model score %s: %.2f%%\" % (model_fish.metrics_names[1], score_fish[1]*100))\n",
    "print(\"Flower model score %s: %.2f%%\" % (model_flower.metrics_names[1], score_flower[1]*100))\n",
    "print(\"Gravel model score %s: %.2f%%\" % (model_gravel.metrics_names[1], score_gravel[1]*100))\n",
    "print(\"Sugar model score %s: %.2f%%\" % (model_sugar.metrics_names[1], score_sugar[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish - max: 0.6771052 , min:  0.6771052 \n",
      " Flower - max: 0.6771052 , min:  0.6771052 \n",
      " Gravel - max: 0.6771052 , min:  0.6771052 \n",
      " Sugar - max: 0.6771052 , min:  0.6771052\n"
     ]
    }
   ],
   "source": [
    "#Looking closer though, we notice that the neural network has built a very simple algorithm that always outputs the same probability\n",
    "print('Fish - max:', np.max(y_pred_dnn_fish), ', min: ', np.min(y_pred_dnn_fish),\n",
    "      '\\n Flower - max:', np.max(y_pred_dnn_flower), ', min: ', np.min(y_pred_dnn_flower),\n",
    "      '\\n Gravel - max:', np.max(y_pred_dnn_gravel), ', min: ', np.min(y_pred_dnn_gravel),\n",
    "      '\\n Sugar - max:', np.max(y_pred_dnn_sugar), ', min: ', np.min(y_pred_dnn_sugar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, defining one classifier per image definitely gives better results than the initial 15-class classifier. Nonetheless, the DNN model appears to use some \"short-cuts\" and ignore the information provided by the features. \n",
    "\\\n",
    "\\\n",
    "We can of course try to fine-tune the model, e.g. by using a different architecture (LeNet) as well as play around with the hyperparameters (number of layers, activiation functions, etc), as well as combining these 4 classifiers into a single classifier. \n",
    "\\\n",
    "Otherwise, we can put the classification models aside and move to the next modeling category where we will reframe the problem into an Object Detection task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
