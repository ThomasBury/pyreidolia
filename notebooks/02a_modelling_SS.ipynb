{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce78cf06",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">‚òÅÔ∏è - Cloudy regions segmentation üë®‚Äçüíªüî¨</h1>\n",
    "\n",
    "<h2 align=\"center\">Modelling - 2st iteration - Semantic Segmentation</h2>\n",
    "<p style=\"text-align:center\">\n",
    "   Thomas Bury, Afonso Alves, Daniel Staudegger<br>\n",
    "   Allianz<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7051b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scicomap as sc\n",
    "import matplotlib as mpl\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#To get a progress bar for long loops:\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "# Custom package for the project, save all the functions into appropriate sub-packages\n",
    "from pyreidolia.plot import set_my_plt_style, plot_cloud, plot_rnd_cloud, draw_label_only\n",
    "from pyreidolia.mask import bounding_box, rle_to_mask, get_binary_mask_sum, mask_to_rle\n",
    "from pyreidolia.img import get_resolution_sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f85dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996106e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport pyreidolia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd829184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer style for mpl\n",
    "set_my_plt_style(height=6, width=8, linewidth=1.5)\n",
    "\n",
    "# A better colormap\n",
    "\n",
    "\n",
    "sc_map = sc.ScicoSequential(cmap='tropical')\n",
    "sc_map.unif_sym_cmap(lift=None, \n",
    "                     bitonic=False, \n",
    "                     diffuse=True)\n",
    "sc_cmap = sc_map.get_mpl_color_map()\n",
    "\n",
    "mpl.cm.register_cmap(\"tropical\", sc_cmap)\n",
    "\n",
    "sc_map = sc.ScicoSequential(cmap='neutral')\n",
    "sc_map.unif_sym_cmap(lift=None, \n",
    "                     bitonic=False, \n",
    "                     diffuse=True)\n",
    "sc_cmap = sc_map.get_mpl_color_map()\n",
    "\n",
    "mpl.cm.register_cmap(\"neutral\", sc_cmap)\n",
    "\n",
    "sc_map = sc.ScicoSequential(cmap='neutral_r')\n",
    "sc_map.unif_sym_cmap(lift=None, \n",
    "                     bitonic=False, \n",
    "                     diffuse=True)\n",
    "sc_cmap = sc_map.get_mpl_color_map()\n",
    "\n",
    "mpl.cm.register_cmap(\"neutral_r\", sc_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67835d",
   "metadata": {},
   "source": [
    "#### Load the config file for the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77072215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_print(df):\n",
    "    return print(df.to_string().replace('\\n', '\\n\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552a9311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the paths.yml config file?C:\\Users\\e400086\\Desktop\\Docs\\DS_Certificate_Sorbonne_and_CV\\02_Project_Cloud_Regions\\paths.yml\n",
      "{'data': {'docs': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/00_Docs_and_Links/',\n",
      "          'test': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/01_Data/test_images/',\n",
      "          'train': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/01_Data/train_images/'},\n",
      " 'notebooks': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/03_Notebooks/',\n",
      " 'output': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/02_Outputs/',\n",
      " 'reports': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/07_Reports/',\n",
      " 'scripts': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/04_Scripts/',\n",
      " 'studies': 'C:/Users/e400086/Desktop/Docs/DS_Certificate_Sorbonne_and_CV/02_Project_Cloud_Regions/06_Studies/'}\n"
     ]
    }
   ],
   "source": [
    "# Where is my yaml ? \"C:/Users/xtbury/Documents/Projects/Pyreidolia/paths.yml\"\n",
    "\n",
    "paths_yml = input(\"where is the paths.yml config file?\")\n",
    "with open(paths_yml, \"r\") as ymlfile:\n",
    "    path_dic = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "pprint(path_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50624f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = path_dic['data']['docs'] + 'train.csv'\n",
    "train_pq_path = path_dic['data']['docs'] + \"train_info_clean.parquet\"\n",
    "train_data = path_dic['data']['train'] \n",
    "test_data = path_dic['data']['test'] \n",
    "report_path = path_dic['reports']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638466e",
   "metadata": {},
   "source": [
    "## Semantic Segmentation modelling\n",
    "### 1) Load the `X_train2` from file\n",
    "**Note:** you need to at least run the _01a_preprocessing.ipynb_ section that generates **X_train2.npy** and **X_valid2.npy**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47dae4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> X_train2 memory size: 5.44 Gb.\n"
     ]
    }
   ],
   "source": [
    "X_train2 = np.load(path_dic['data']['docs'] +'X_train2.npy')\n",
    "print(\">> X_train2 memory size: %.2f Gb.\"\n",
    "      % (X_train2.nbytes*1.0*10**(-9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64d65c",
   "metadata": {},
   "source": [
    "* We don't need to load `X_valid2` at the moment; avoid doing it to reduce memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59460bd1",
   "metadata": {},
   "source": [
    "### 2.a) Modelling - Fish cloud\n",
    "The plan is to train 4 separate NN, each trying to predict a different cloud label. We should only load into memory data that we need, and delete data that's not being used.  \n",
    "**Load the target arrays of Fish (total of 42):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1ebfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(row_px_data, col_px_data): 700 , 467\n",
      "(row_px_target, col_px_target): 420 , 280\n"
     ]
    }
   ],
   "source": [
    "row_px_data, col_px_data = 700, 467\n",
    "print(\"(row_px_data, col_px_data):\", row_px_data,\",\", col_px_data)\n",
    "row_px_target, col_px_target = 420, 280\n",
    "print(\"(row_px_target, col_px_target):\", row_px_target,\",\", col_px_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84687129",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2_SS_Fish = np.empty((0, row_px_target*col_px_target))\n",
    "\n",
    "for section_number in range(0,41+1):\n",
    "    #Load from local memory:\n",
    "    target_section = np.load(path_dic['data']['docs'] +'y_train2_SS_Fish_'+str(section_number)+'.npy')\n",
    "    #Add to main array:\n",
    "    y_train2_SS_Fish = np.append(y_train2_SS_Fish, target_section, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8857483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> y_train2_SS_Fish memory size: 3.91 Gb.\n"
     ]
    }
   ],
   "source": [
    "print(\">> y_train2_SS_Fish memory size: %.2f Gb.\"\n",
    "      % (y_train2_SS_Fish.nbytes*1.0*10**(-9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a6379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> y_train2_SS_Fish shape: (4160, 117600).\n"
     ]
    }
   ],
   "source": [
    "print(\">> y_train2_SS_Fish shape: \"+ str(y_train2_SS_Fish.shape)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b59ecd",
   "metadata": {},
   "source": [
    "**Transform the data into a 4-dimensional array (nb_images, width, height, depth)**. Each of the images will be resized to (row_px, col_px, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4590ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of X_train2: (4160, 700, 467, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train2 = X_train2.reshape((-1, row_px_data, col_px_data, 1))\n",
    "print('New shape of X_train2:', X_train2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032aa36",
   "metadata": {},
   "source": [
    "**LeNet neural network - create layer structure:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83f9e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=Input(shape = (700, 467, 1), name = \"Input\")\n",
    "\n",
    "conv_1 = Conv2D(filters = 30,\n",
    "                kernel_size = (5, 5),\n",
    "                padding = 'valid',\n",
    "                input_shape = (700, 467, 1),\n",
    "                activation = 'relu')\n",
    "\n",
    "max_pool_1 = MaxPooling2D(pool_size = (2, 2))\n",
    "\n",
    "conv_2 = Conv2D(filters = 30,                    \n",
    "                kernel_size = (3, 3),          \n",
    "                padding = 'valid',             \n",
    "                activation = 'relu')\n",
    "\n",
    "max_pool_2 = MaxPooling2D(pool_size = (2, 2))\n",
    "\n",
    "flatten = Flatten()\n",
    "\n",
    "dropout = Dropout(rate = 0.2)\n",
    "\n",
    "dense_1 = Dense(units = 2000, activation = 'relu')\n",
    "dense_2 = Dense(units = 1000, activation = 'relu')\n",
    "dense_3 = Dense(units = 500, activation = 'relu')\n",
    "\n",
    "dense_4 = Dense(units = row_px_target* col_px_target, activation = 'softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c1670",
   "metadata": {},
   "source": [
    "**Add layers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aaa2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=conv_1(inputs)\n",
    "x=max_pool_1(x)\n",
    "x=conv_2(x)\n",
    "x=max_pool_2(x)\n",
    "\n",
    "x=dropout(x)\n",
    "x=flatten(x)\n",
    "x=dense_1(x)\n",
    "x=dense_2(x)\n",
    "x=dense_3(x)\n",
    "outputs=dense_4(x)\n",
    "\n",
    "lenet = Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ffcf2",
   "metadata": {},
   "source": [
    "**Compile:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "719d5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "lenet.compile(loss='BinaryCrossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c74914",
   "metadata": {},
   "source": [
    "**Fit:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e7f33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 639, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 830, in _create_all_weights\n        self._create_slots(var_list)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\adam.py\", line 119, in _create_slots\n        self.add_slot(var, 'v')\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 916, in add_slot\n        weight = tf.Variable(\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\initializers\\initializers_v2.py\", line 144, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[591660,2000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18564/2859772302.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m training_history_lenet = lenet.fit(X_train2, y_train2_SS_Fish,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                    \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                    \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                    batch_size = 25)\n",
      "\u001b[1;32m~\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pyreidolia\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\engine\\training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 639, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 830, in _create_all_weights\n        self._create_slots(var_list)\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\adam.py\", line 119, in _create_slots\n        self.add_slot(var, 'v')\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 916, in add_slot\n        weight = tf.Variable(\n    File \"C:\\Users\\e400086\\.conda\\envs\\pyreidolia\\lib\\site-packages\\keras\\initializers\\initializers_v2.py\", line 144, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[591660,2000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "training_history_lenet = lenet.fit(X_train2, y_train2_SS_Fish,\n",
    "                                   validation_split = 0.2,\n",
    "                                   epochs = 2,\n",
    "                                   batch_size = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41dfe278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07d5a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> X_valid2 memory size: 1.81 Gb.\n"
     ]
    }
   ],
   "source": [
    "X_valid2 = np.load(path_dic['data']['docs'] +'X_valid2.npy')\n",
    "print(\">> X_valid2 memory size: %.2f Gb.\"\n",
    "      % (X_valid2.nbytes*1.0*10**(-9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db2aaee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_valid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c1d82a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18564/3635118219.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2_SS_Fish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train2' is not defined"
     ]
    }
   ],
   "source": [
    "del X_train2, y_train2_SS_Fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f63f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
